{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# WebShop LATs Experiment Notebook\n",
        "\n",
        "This notebook replicates the functionality of `run.py` for running WebShop experiments using Language Agent Tree Search (LATs).\n",
        "\n",
        "## Overview\n",
        "- **Task**: WebShop shopping environment\n",
        "- **Method**: Language Agent Tree Search (LATs)\n",
        "- **Models**: GPT-3.5, GPT-4, or other supported backends\n",
        "- **Features**: Self-reflection, trajectory learning, and iterative improvement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import sys\n",
        "import copy\n",
        "import itertools\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "import requests\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Comment\n",
        "import backoff\n",
        "import openai\n",
        "from transformers import GPT2Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add the current directory to Python path to import local modules\n",
        "webshop_path = os.path.abspath(os.path.join(os.getcwd(), '../LanguageAgentTreeSearch/webshop'))\n",
        "sys.path.append(webshop_path)\n",
        "\n",
        "# Import local modules\n",
        "from models import LiteLLMModel\n",
        "from webshop import WebShopTask\n",
        "\n",
        "# # Import specific functions from lats.py that we need\n",
        "from lats import (\n",
        "    select_node, expand_node, rollout, backpropagate, \n",
        "    collect_all_nodes, Node, env, evaluate_node\n",
        ")\n",
        "\n",
        "print(\"All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration and Setup\n",
        "\n",
        "Configure your experiment parameters here. You can modify these values to run different experiments.\n",
        "\n",
        "**Note**: This notebook uses LiteLLM, which requires the `TOGETHER_API_KEY` environment variable to be set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ TOGETHER_API_KEY is set\n",
            "\n",
            "Experiment Configuration:\n",
            "Backend: together_ai/Qwen/Qwen3-Next-80B-A3B-Instruct\n",
            "Temperature: 1.0\n",
            "Max Tokens: 4096\n",
            "Task range: 900 to 1000\n",
            "Prompt sample: standard\n",
            "Iterations: 30\n",
            "Log file: webshop_experiment_20251012_160826.log\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv(override=True)\n",
        "\n",
        "# Check environment variables\n",
        "if not os.getenv(\"TOGETHER_API_KEY\"):\n",
        "    print(\"WARNING: TOGETHER_API_KEY environment variable is not set!\")\n",
        "    print(\"Please set it before running the experiment:\")\n",
        "    print(\"export TOGETHER_API_KEY='your_api_key_here'\")\n",
        "else:\n",
        "    print(\"âœ“ TOGETHER_API_KEY is set\")\n",
        "\n",
        "# Configuration parameters - modify these as needed\n",
        "class ExperimentConfig:\n",
        "    def __init__(self):\n",
        "        # Model configuration - using LiteLLM model names\n",
        "        self.backend = \"together_ai/Qwen/Qwen3-Next-80B-A3B-Instruct\"\n",
        "        self.temperature = 1.0\n",
        "        self.max_tokens = 4096\n",
        "        \n",
        "        # Task configuration\n",
        "        self.task_start_index = 900\n",
        "        self.task_end_index = 1000\n",
        "        \n",
        "        # Sampling configuration\n",
        "        self.prompt_sample = 'standard'  # Options: 'standard', 'cot'\n",
        "        self.n_generate_sample = 1\n",
        "        self.n_evaluate_sample = 1\n",
        "        \n",
        "        # Search configuration\n",
        "        self.iterations = 30\n",
        "        \n",
        "        # Logging configuration\n",
        "        self.log_file = f'webshop_experiment_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
        "        \n",
        "        # Display configuration\n",
        "        self.print_progress = True\n",
        "\n",
        "# Create configuration instance\n",
        "config = ExperimentConfig()\n",
        "\n",
        "print(\"\\nExperiment Configuration:\")\n",
        "print(f\"Backend: {config.backend}\")\n",
        "print(f\"Temperature: {config.temperature}\")\n",
        "print(f\"Max Tokens: {config.max_tokens}\")\n",
        "print(f\"Task range: {config.task_start_index} to {config.task_end_index}\")\n",
        "print(f\"Prompt sample: {config.prompt_sample}\")\n",
        "print(f\"Iterations: {config.iterations}\")\n",
        "print(f\"Log file: {config.log_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing LiteLLM model...\n",
            "LiteLLM model initialized: together_ai/Qwen/Qwen3-Next-80B-A3B-Instruct\n",
            "WebShop Task initialized:\n",
            "<webshop.WebShopTask object at 0x15b88c690>\n"
          ]
        }
      ],
      "source": [
        "# Initialize the LiteLLM model\n",
        "print(\"Initializing LiteLLM model...\")\n",
        "model = LiteLLMModel(\n",
        "    model=config.backend,\n",
        "    temperature=config.temperature,\n",
        "    max_tokens=config.max_tokens\n",
        ")\n",
        "print(f\"LiteLLM model initialized: {config.backend}\")\n",
        "\n",
        "# Initialize the WebShop task\n",
        "task = WebShopTask()\n",
        "print(\"WebShop Task initialized:\")\n",
        "print(task)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing LiteLLM wrapper...\n",
            "Test response: Hello! It looks like you're testing the watersâ€”welcome! ðŸ˜Š  \n",
            "How can I assist you today? Whether itâ€™s...\n",
            "LiteLLM wrapper functions created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Create wrapper functions to make LiteLLM compatible with existing code\n",
        "def gpt(prompt, model=None, temperature=None, max_tokens=None, n=1, stop=None):\n",
        "    \"\"\"\n",
        "    Wrapper function to make LiteLLM compatible with the existing gpt function interface\n",
        "    \"\"\"\n",
        "    if isinstance(prompt, str):\n",
        "        # Single prompt\n",
        "        if n == 1:\n",
        "            return [model.send_request(prompt)]\n",
        "        else:\n",
        "            # Multiple samples of the same prompt\n",
        "            prompts = [prompt] * n\n",
        "            return model.send_requests(prompts)\n",
        "    else:\n",
        "        # List of prompts\n",
        "        return model.send_requests(prompt)\n",
        "\n",
        "def gpt_usage(backend=None):\n",
        "    \"\"\"\n",
        "    Wrapper function for usage tracking - LiteLLM doesn't have built-in usage tracking\n",
        "    so we'll return a placeholder structure\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'completion_tokens': 0,\n",
        "        'prompt_tokens': 0,\n",
        "        'cost': 0.0\n",
        "    }\n",
        "\n",
        "# Test the wrapper\n",
        "print(\"Testing LiteLLM wrapper...\")\n",
        "test_prompt = \"Hello, this is a test prompt.\"\n",
        "test_response = gpt(test_prompt, model=model, n=1)\n",
        "print(f\"Test response: {test_response[0][:100]}...\")\n",
        "print(\"LiteLLM wrapper functions created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Redefine lats_search function to work with LiteLLM (no globals!)\n",
        "def lats_search_with_litellm(args, task, idx, model, iterations=50, to_print=True):\n",
        "    \"\"\"\n",
        "    Clean lats_search function that works with LiteLLM model - no global variables!\n",
        "    \"\"\"\n",
        "    # Create a gpt function that uses our LiteLLM model\n",
        "    def gpt_with_litellm(prompt, n=1, stop=None):\n",
        "        return gpt(prompt, model=model, n=n, stop=stop)\n",
        "    \n",
        "    # Set up the environment\n",
        "    action = 'reset'\n",
        "    logging.basicConfig(filename=args.log, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', filemode='a')\n",
        "    \n",
        "    x = env.step(idx, action)[0]\n",
        "    if to_print:\n",
        "        print(idx, x)\n",
        "    \n",
        "    root = Node(state=None, question=x)\n",
        "    root.env_state = copy.deepcopy(env.sessions)\n",
        "    all_nodes = []\n",
        "    failed_trajectories = []  # Local variable, not global\n",
        "    reflection_map = []        # Local variable, not global\n",
        "    terminal_nodes = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "        logging.info(f\"Iteration {i + 1}...\")\n",
        "        node = select_node(root)\n",
        "\n",
        "        while node is None or (node.is_terminal and node.reward != 1):\n",
        "            logging.info(f\"Need to backtrack or terminal node with reward 0 found at iteration {i + 1}, reselecting...\")\n",
        "            node = select_node(root)\n",
        "        \n",
        "        if node is None:\n",
        "            logging.info(\"All paths lead to terminal nodes with reward 0. Ending search.\")\n",
        "            break\n",
        "\n",
        "        if node.is_terminal and node.reward == 1:\n",
        "            logging.info(f\"Terminal node with reward 1 found at iteration {i + 1}\")\n",
        "            return node.state, node.value, all_nodes, node.reward, node.em\n",
        "        \n",
        "        expand_node(node, args, task, idx)\n",
        "\n",
        "        while node.is_terminal:\n",
        "            logging.info(f\"Depth limit node found at iteration {i + 1}, reselecting...\")\n",
        "            node = select_node(root)\n",
        "            expand_node(node, args, task, idx)\n",
        "\n",
        "        val = evaluate_node(node, args, task, idx)\n",
        "        # Simulation or rollout\n",
        "        terminal_node = rollout(max(node.children, key=lambda child: child.value), args, task, idx, max_depth=15)\n",
        "        terminal_nodes.append(terminal_node)\n",
        "\n",
        "        if terminal_node.reward == 1:\n",
        "            logging.info(\"Successful trajectory found\")\n",
        "            logging.info(f\"Terminal node with reward 1 found at iteration {i + 1}\")\n",
        "            return terminal_node.state, terminal_node.value, terminal_node.reward, terminal_node.em\n",
        "        # Backpropagate reward\n",
        "        backpropagate(terminal_node, terminal_node.reward)\n",
        "        \n",
        "        all_nodes = [(node, node.reward) for node in collect_all_nodes(root)]\n",
        "        print(\"searching all nodes...\")\n",
        "        # Check for terminal nodes with a reward of 1\n",
        "        terminal_nodes_with_reward_1 = [node for node, reward in all_nodes if node.is_terminal and node.reward == 1]\n",
        "\n",
        "        if terminal_nodes_with_reward_1:\n",
        "            logging.info(\"Successful trajectory found\")\n",
        "            logging.info(f\"Terminal node with reward 1 found at iteration {i + 1}\")\n",
        "            best_node = max(terminal_nodes_with_reward_1, key=lambda x: x.reward)\n",
        "            return best_node.state, best_node.value, best_node.reward, best_node.em\n",
        "    \n",
        "        for j, (node, value) in enumerate(all_nodes):\n",
        "            logging.info(f\"Node {j+1}: {str(node)}\")\n",
        "\n",
        "        node_strings = '\\n'.join(str(node[0]) for node in all_nodes)\n",
        "        logging.info(f\"State of all_nodes after iteration {i + 1}:\\n{node_strings}\")\n",
        "\n",
        "    #best_child = max(root.children, key=lambda x: x.reward)\n",
        "    all_nodes_list = collect_all_nodes(root)\n",
        "    all_nodes_list.extend(terminal_nodes)\n",
        "    best_child = max(all_nodes_list, key=lambda x: x.reward)\n",
        "    print(\"best value found\", best_child.reward)\n",
        "    if best_child.reward == 1:\n",
        "        logging.info(\"Successful trajectory found\")\n",
        "    else:\n",
        "        logging.info(\"Unsuccessful/Partially Successful trajectory found\")\n",
        "    return best_child.state, best_child.value, best_child.reward, best_child.em\n",
        "\n",
        "print(\"Clean lats_search function defined with LiteLLM support - no globals!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    filename=config.log_file, \n",
        "    level=logging.INFO, \n",
        "    format='%(asctime)s - %(levelname)s - %(message)s', \n",
        "    filemode='a'\n",
        ")\n",
        "\n",
        "print(f\"Logging configured. Log file: {config.log_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main experiment function - replicates the functionality of run.py\n",
        "def run_experiment(config):\n",
        "    \"\"\"\n",
        "    Run the WebShop experiment with LATs search.\n",
        "    This function replicates the main logic from run.py\n",
        "    \"\"\"\n",
        "    # Initialize tracking variables\n",
        "    logs = []\n",
        "    task_accs = []\n",
        "    info = []\n",
        "    count = 0\n",
        "    n = config.task_end_index - config.task_start_index\n",
        "    \n",
        "    print(f\"Starting experiment with {n} tasks...\")\n",
        "    print(f\"Task range: {config.task_start_index} to {config.task_end_index}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Create a simple args object for compatibility with lats_search\n",
        "    class Args:\n",
        "        def __init__(self, config):\n",
        "            self.backend = config.backend\n",
        "            self.temperature = config.temperature\n",
        "            self.prompt_sample = config.prompt_sample\n",
        "            self.n_generate_sample = config.n_generate_sample\n",
        "            self.n_evaluate_sample = config.n_evaluate_sample\n",
        "            self.iterations = config.iterations\n",
        "            self.log = config.log_file\n",
        "    \n",
        "    args = Args(config)\n",
        "    \n",
        "    # Main experiment loop\n",
        "    for i in range(config.task_start_index, config.task_end_index):\n",
        "        print(f\"\\n--- Task {i+1} (index {i}) ---\")\n",
        "        \n",
        "        try:\n",
        "            # Run LATs search for this task using our custom function\n",
        "            state, value, reward, em = lats_search_with_litellm(args, task, f'fixed_{i}', model, config.iterations, config.print_progress)\n",
        "            \n",
        "            # Track results\n",
        "            task_accs.append(reward)\n",
        "            print(f\"Task {i+1} - Best reward: {reward}\")\n",
        "            \n",
        "            # Print progress every task\n",
        "            if (i+1) % 1 == 0:\n",
        "                r = sum(task_accs) / len(task_accs)  # Average reward\n",
        "                sr = len([_ for _ in task_accs if _ == 1]) / len(task_accs)  # Success rate\n",
        "                fr = count / len(task_accs)  # Failure rate\n",
        "                print(f\"Progress - Task {i+1}: Avg Reward: {r:.3f}, Success Rate: {sr:.3f}, Failure Rate: {fr:.3f}\")\n",
        "                print('-' * 30)\n",
        "            \n",
        "            # Log results\n",
        "            r, sr, fr = sum(task_accs) / len(task_accs), len([_ for _ in task_accs if _ == 1]) / n, count / n\n",
        "            logging.info(f\"RESULTS: {r}, {sr}, {fr}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error in task {i+1}: {str(e)}\")\n",
        "            logging.error(f\"Error in task {i+1}: {str(e)}\")\n",
        "            task_accs.append(0)  # Add 0 reward for failed task\n",
        "            count += 1\n",
        "    \n",
        "    # Final results\n",
        "    n = config.task_end_index - config.task_start_index\n",
        "    final_r = sum(task_accs) / len(task_accs) if task_accs else 0\n",
        "    final_sr = len([_ for _ in task_accs if _ == 1]) / n if n > 0 else 0\n",
        "    final_fr = count / n if n > 0 else 0\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"FINAL RESULTS:\")\n",
        "    print(f\"Average Reward: {final_r:.3f}\")\n",
        "    print(f\"Success Rate: {final_sr:.3f}\")\n",
        "    print(f\"Failure Rate: {final_fr:.3f}\")\n",
        "    print(f\"Total Tasks: {n}\")\n",
        "    # Note: LiteLLM doesn't provide built-in usage tracking\n",
        "    usage_info = gpt_usage(config.backend)\n",
        "    print(f\"Usage tracking: {usage_info}\")\n",
        "    \n",
        "    return {\n",
        "        'task_accs': task_accs,\n",
        "        'final_r': final_r,\n",
        "        'final_sr': final_sr,\n",
        "        'final_fr': final_fr,\n",
        "        'usage': usage_info\n",
        "    }\n",
        "\n",
        "print(\"Experiment function defined. Ready to run!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the experiment\n",
        "print(\"Starting WebShop LATs Experiment...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Run the experiment and capture results\n",
        "results = run_experiment(config)\n",
        "\n",
        "print(\"\\nExperiment completed!\")\n",
        "print(\"Results saved to:\", results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze and visualize results\n",
        "def analyze_results(results):\n",
        "    \"\"\"\n",
        "    Analyze the experiment results and create visualizations\n",
        "    \"\"\"\n",
        "    task_accs = results['task_accs']\n",
        "    \n",
        "    # Create a DataFrame for analysis\n",
        "    df = pd.DataFrame({\n",
        "        'Task': range(len(task_accs)),\n",
        "        'Reward': task_accs,\n",
        "        'Success': [1 if r == 1.0 else 0 for r in task_accs]\n",
        "    })\n",
        "    \n",
        "    print(\"=== EXPERIMENT ANALYSIS ===\")\n",
        "    print(f\"Total Tasks: {len(task_accs)}\")\n",
        "    print(f\"Average Reward: {results['final_r']:.3f}\")\n",
        "    print(f\"Success Rate: {results['final_sr']:.3f}\")\n",
        "    print(f\"Failure Rate: {results['final_fr']:.3f}\")\n",
        "    print(f\"Max Reward: {max(task_accs):.3f}\")\n",
        "    print(f\"Min Reward: {min(task_accs):.3f}\")\n",
        "    \n",
        "    # Usage information\n",
        "    usage = results['usage']\n",
        "    print(f\"\\n=== USAGE STATISTICS ===\")\n",
        "    print(f\"Completion Tokens: {usage['completion_tokens']:,}\")\n",
        "    print(f\"Prompt Tokens: {usage['prompt_tokens']:,}\")\n",
        "    print(f\"Total Cost: ${usage['cost']:.4f}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Analyze results\n",
        "if 'results' in locals():\n",
        "    df = analyze_results(results)\n",
        "else:\n",
        "    print(\"No results available yet. Run the experiment first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations\n",
        "def create_visualizations(df):\n",
        "    \"\"\"\n",
        "    Create visualizations of the experiment results\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # 1. Reward over tasks\n",
        "    axes[0, 0].plot(df['Task'], df['Reward'], 'b-', alpha=0.7, linewidth=1)\n",
        "    axes[0, 0].set_title('Reward per Task')\n",
        "    axes[0, 0].set_xlabel('Task Number')\n",
        "    axes[0, 0].set_ylabel('Reward')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Success rate over time (cumulative)\n",
        "    df['Cumulative_Success'] = df['Success'].cumsum()\n",
        "    df['Cumulative_Success_Rate'] = df['Cumulative_Success'] / (df['Task'] + 1)\n",
        "    axes[0, 1].plot(df['Task'], df['Cumulative_Success_Rate'], 'g-', linewidth=2)\n",
        "    axes[0, 1].set_title('Cumulative Success Rate')\n",
        "    axes[0, 1].set_xlabel('Task Number')\n",
        "    axes[0, 1].set_ylabel('Success Rate')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Reward distribution\n",
        "    axes[1, 0].hist(df['Reward'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[1, 0].set_title('Reward Distribution')\n",
        "    axes[1, 0].set_xlabel('Reward')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Success vs Failure\n",
        "    success_count = df['Success'].sum()\n",
        "    failure_count = len(df) - success_count\n",
        "    axes[1, 1].pie([success_count, failure_count], \n",
        "                   labels=['Success', 'Failure'], \n",
        "                   autopct='%1.1f%%',\n",
        "                   colors=['lightgreen', 'lightcoral'])\n",
        "    axes[1, 1].set_title('Success vs Failure')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Create visualizations if results are available\n",
        "if 'results' in locals() and 'df' in locals():\n",
        "    df_with_viz = create_visualizations(df)\n",
        "else:\n",
        "    print(\"No results available for visualization. Run the experiment first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to file\n",
        "def save_results(results, filename=None):\n",
        "    \"\"\"\n",
        "    Save experiment results to a JSON file\n",
        "    \"\"\"\n",
        "    if filename is None:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"webshop_results_{timestamp}.json\"\n",
        "    \n",
        "    # Prepare results for JSON serialization\n",
        "    save_data = {\n",
        "        'config': {\n",
        "            'backend': config.backend,\n",
        "            'temperature': config.temperature,\n",
        "            'task_start_index': config.task_start_index,\n",
        "            'task_end_index': config.task_end_index,\n",
        "            'prompt_sample': config.prompt_sample,\n",
        "            'iterations': config.iterations\n",
        "        },\n",
        "        'results': {\n",
        "            'task_accs': results['task_accs'],\n",
        "            'final_r': results['final_r'],\n",
        "            'final_sr': results['final_sr'],\n",
        "            'final_fr': results['final_fr'],\n",
        "            'usage': results['usage']\n",
        "        },\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(save_data, f, indent=2)\n",
        "    \n",
        "    print(f\"Results saved to: {filename}\")\n",
        "    return filename\n",
        "\n",
        "# Save results if available\n",
        "if 'results' in locals():\n",
        "    results_file = save_results(results)\n",
        "else:\n",
        "    print(\"No results to save. Run the experiment first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Start Guide\n",
        "\n",
        "### Prerequisites:\n",
        "1. Set the `TOGETHER_API_KEY` environment variable\n",
        "2. Make sure you have the required dependencies installed\n",
        "\n",
        "### To run a single task experiment:\n",
        "```python\n",
        "# Modify config for a single task\n",
        "config.task_start_index = 900\n",
        "config.task_end_index = 901\n",
        "config.iterations = 10  # Reduce iterations for faster testing\n",
        "\n",
        "# Run the experiment\n",
        "results = run_experiment(config)\n",
        "```\n",
        "\n",
        "### To run a larger experiment:\n",
        "```python\n",
        "# Modify config for multiple tasks\n",
        "config.task_start_index = 900\n",
        "config.task_end_index = 910  # 10 tasks\n",
        "config.iterations = 30\n",
        "\n",
        "# Run the experiment\n",
        "results = run_experiment(config)\n",
        "```\n",
        "\n",
        "### To change the model:\n",
        "```python\n",
        "# Change the backend model (LiteLLM supports many models)\n",
        "config.backend = 'gpt-4'  # or 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'claude-3-sonnet', etc.\n",
        "config.temperature = 0.7  # Adjust temperature\n",
        "config.max_tokens = 4096  # Adjust max tokens\n",
        "```\n",
        "\n",
        "### To use Chain-of-Thought prompting:\n",
        "```python\n",
        "# Enable CoT prompting\n",
        "config.prompt_sample = 'cot'\n",
        "```\n",
        "\n",
        "### Note about LiteLLM:\n",
        "This notebook uses LiteLLM for model access, which provides:\n",
        "- Support for multiple model providers\n",
        "- Automatic retry logic\n",
        "- Concurrent request handling\n",
        "- Unified interface across different models"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
